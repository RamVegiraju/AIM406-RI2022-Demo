{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b578064",
   "metadata": {},
   "source": [
    "## AIM 406: Reinvent 2022 Notebook\n",
    "### TensorFlow U-Net Optimization\n",
    " \n",
    "<b>Credits</b>: This is an adaptation and extension of the following [example notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/aws_sagemaker_studio/sagemaker_neo_compilation_jobs/tensorflow_unet/sagemaker-neo-tf-unet.ipynb) from SageMaker Examples.\n",
    "\n",
    "The notebook contains the following performance optimization techniques\n",
    "- TensorFlow Serving Container Environment Variable Tuning\n",
    "- Neo Compilation\n",
    "- Load Testing & AutoScaling\n",
    "\n",
    "Note that distributed locust test will most likely crash whatever Notebook Instance you are using. It is recommended to run this script on EC2 or Kubernetes, so that the client side has enough workers/power to handle the distributed test. For the purpose of the demo we will be running the test on an EC2 instance of type c6i.32xlarge. To get a better understanding on setting up an EC2 instance and the Locust load test please reference this repository's [README](https://github.com/aws-samples/load-testing-sagemaker-endpoints).\n",
    "\n",
    "- Notebook Setting:\n",
    "    - Kernel: conda_tensorflow2_p38\n",
    "    - Notebook Instance: ml.g4dn.12xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84882fb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --quiet --upgrade \"sagemaker\"\n",
    "!pip install -U --quiet \"tensorflow==1.15.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e499251",
   "metadata": {},
   "source": [
    "### Retreive Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unet_medical\"\n",
    "export_path = \"export\"\n",
    "model_archive_name = \"unet-medical.tar.gz\"\n",
    "model_archive_url = \"https://sagemaker-neo-artifacts.s3.us-east-2.amazonaws.com/{}\".format(\n",
    "    model_archive_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {model_archive_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e194d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf unet-medical.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8080e",
   "metadata": {},
   "source": [
    "Understand model input/output data formats for Neo compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = os.path.join(export_path, \"Servo/1\")\n",
    "!saved_model_cli show --all --dir {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = Session().upload_data(path=model_archive_name, key_prefix=\"model\")\n",
    "print(\"model uploaded to: {}\".format(model_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcfe19",
   "metadata": {},
   "source": [
    "## Endpoint Creation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "instance_type = \"ml.g4dn.16xlarge\"\n",
    "framework = \"TENSORFLOW\"\n",
    "framework_version = \"1.15.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a61d0",
   "metadata": {},
   "source": [
    "### Environment Variable Tuning\n",
    "\n",
    "Depending on the framework you are working with there are a number of container level variables that you can tune. For TensorFlow reference the following [blog](https://aws.amazon.com/blogs/machine-learning/maximize-tensorflow-performance-on-amazon-sagemaker-endpoints-for-real-time-inference/) and [serving code](https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/docker/build_artifacts/sagemaker/serve.py#L40-L70). For this example we tune the following environment variable to maximize the throughput our endpoint can achieve.\n",
    "\n",
    "- SageMaker Gunicorn Workers\n",
    "- SAGEMAKER_TFS_INTER_OP_PARALLELISM\n",
    "- SAGEMAKER_TFS_INTRA_OP_PARALLELISM\n",
    "- SAGEMAKER_TFS_INSTANCE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model = Model(model_data=model_data, framework_version=framework_version, role=role,\n",
    "                 env= {\n",
    "                    'SAGEMAKER_GUNICORN_WORKERS': '64',\n",
    "                    'SAGEMAKER_TFS_INTER_OP_PARALLELISM': '1',\n",
    "                    'SAGEMAKER_TFS_INTRA_OP_PARALLELISM': '1',\n",
    "                    'SAGEMAKER_TFS_INSTANCE_COUNT': '8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f372d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_predictor = sm_model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ead940",
   "metadata": {},
   "source": [
    "### Payload Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_fname = \"cell-4.png\"\n",
    "sample_img_url = \"https://sagemaker-neo-artifacts.s3.us-east-2.amazonaws.com/{}\".format(\n",
    "    sample_img_fname\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48002b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {sample_img_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21053955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the image file into a tensor (numpy array)\n",
    "!pip install --quiet opencv-python\n",
    "#!apt-get update -q && apt-get install ffmpeg libsm6 libxext6  -y -q\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread(sample_img_fname)\n",
    "original_shape = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.resize(image, (256, 256, 3))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = np.asarray(image)\n",
    "image = np.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = json.dumps(image.tolist()) #serialize payload to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e1359",
   "metadata": {},
   "source": [
    "### Sample Inference with Boto3 and SageMaker SDK\n",
    "\n",
    "For the difference between the two SDKs please reference this [article](https://towardsdatascience.com/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192959ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "imgetsizeofson\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'tensorflow-inference-2022-11-26-03-09-03-293' #replace with your endpoint name\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# get a prediction from the endpoint\n",
    "# the image input is automatically converted to a JSON request.\n",
    "# the JSON response from the endpoint is returned as a python dict\n",
    "result = uncompiled_predictor.predict(image)\n",
    "print(\"Prediction took %.2f seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee942f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_results = []\n",
    "endpoint_name = 'tensorflow-inference-2022-11-26-03-09-03-293'\n",
    "\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)\n",
    "    uncompiled_results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for un-compiled model: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(uncompiled_results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(uncompiled_results, 90)) + \" ms\\n\")\n",
    "print(\"P50: \" + str(np.percentile(uncompiled_results, 50)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(uncompiled_results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bdf84",
   "metadata": {},
   "source": [
    "## Neo Compilation and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the value of data_shape below and\n",
    "# specify the name & shape of the expected inputs for your trained model in JSON\n",
    "# Note that -1 is replaced with 1 for the batch size placeholder\n",
    "data_shape = {\"inputs\": [1, 224, 224, 3]}\n",
    "\n",
    "instance_family = \"ml_g4dn\"\n",
    "\n",
    "compilation_job_name = name_from_base(\"medical-tf-Neo\")\n",
    "# output path for compiled model artifact\n",
    "compiled_model_path = \"s3://{}/{}/output\".format(bucket, compilation_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_estimator = sm_model.compile(\n",
    "    target_instance_family=instance_family,\n",
    "    input_shape=data_shape,\n",
    "    job_name=compilation_job_name,\n",
    "    role=role,\n",
    "    framework=framework.lower(),\n",
    "    framework_version=framework_version,\n",
    "    output_path=compiled_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# get a prediction from the endpoint\n",
    "# the image input is automatically converted to a JSON request.\n",
    "# the JSON response from the endpoint is returned as a python dict\n",
    "result = optimized_predictor.predict(image)\n",
    "print(\"Prediction took %.2f seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77803f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_results = []\n",
    "#test_input = {\"instances\": np.asarray(shape_input).tolist()}\n",
    "# Warmup inference.\n",
    "optimized_predictor.predict(image)\n",
    "# Inferencing 100 times.\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    optimized_predictor.predict(image)\n",
    "    compiled_results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for compiled model: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(compiled_results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(compiled_results, 90)) + \" ms\\n\")\n",
    "print(\"P50: \" + str(np.percentile(compiled_results, 50)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(compiled_results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'tensorflow-inference-ml-g4dn-2022-11-26-03-21-16-793' #replace with compiled endpoint name\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'tensorflow-inference-ml-g4dn-2022-11-26-03-21-16-793'\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)\n",
    "    compiled_results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for compiled model: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(compiled_results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(compiled_results, 90)) + \" ms\\n\")\n",
    "print(\"P50: \" + str(np.percentile(compiled_results, 50)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(compiled_results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30304b1f",
   "metadata": {},
   "source": [
    "## AutoScaling\n",
    "\n",
    "For a full AutoScaling breakdown, reference this [blog](https://towardsdatascience.com/autoscaling-sagemaker-real-time-endpoints-b1b6e6731c59) and [code samples](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/AdvancedFunctionality/AutoScaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoScaling client\n",
    "asg = boto3.client('application-autoscaling')\n",
    "endpoint_name = \n",
    "\n",
    "# Resource type is variant and the unique identifier is the resource ID.\n",
    "resource_id=f\"endpoint/{endpoint_name}/variant/AllTraffic\"\n",
    "\n",
    "# scaling configuration\n",
    "response = asg.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")\n",
    "\n",
    "#Target Scaling: Once invocations reach 10, the instance will scale out to 4 within 30 seconds and cool back down in 500 seconds.\n",
    "response = asg.put_scaling_policy(\n",
    "    PolicyName=f'Request-ScalingPolicy-{endpoint_name}',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # Threshold\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 500, # duration until scale in\n",
    "        'ScaleOutCooldown': 30 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b889a9b",
   "metadata": {},
   "source": [
    "Sending requests for a certain duration so we can capture the AutoScaling of the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_duration = 400\n",
    "endpoint_name='tensorflow-inference-2022-11-14-20-31-52-879'\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"test will run for {request_duration} seconds\")\n",
    "while time.time() < end_time:\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image',     \n",
    "                                   Body=birds_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417261e",
   "metadata": {},
   "source": [
    "We can monitor the endpoint scaling out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    instance_count = response['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c0564",
   "metadata": {},
   "source": [
    "### Write payload to S3 for Load Testing with Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e68980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the payload to a text file for our locust script to read\n",
    "text_file = open(\"unet-payload.txt\", \"w\")\n",
    "n = text_file.write(payload)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open('unet-payload.txt', 'r')\n",
    "text_payload = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8774e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'tensorflow-inference-ml-g4dn-2022-11-26-03-21-16-793'\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=text_payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp unet-payload.txt s3://{bucket}/unet-payload.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7b22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0bcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
